{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529b3093",
   "metadata": {},
   "source": [
    "## Data gathering - Negative Class only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28dfe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClusterID</th>\n",
       "      <th>DetectionDate_first</th>\n",
       "      <th>DetectionDate_last</th>\n",
       "      <th>Duration</th>\n",
       "      <th>avgLatitude</th>\n",
       "      <th>avgLongitude</th>\n",
       "      <th>totalArea_km2</th>\n",
       "      <th>maxBrightness</th>\n",
       "      <th>maxBright_t31</th>\n",
       "      <th>maxFRP</th>\n",
       "      <th>avgConfidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2132</td>\n",
       "      <td>2000-28-12</td>\n",
       "      <td>2000-28-12</td>\n",
       "      <td>0</td>\n",
       "      <td>63.098125</td>\n",
       "      <td>-162.816950</td>\n",
       "      <td>0.979713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2133</td>\n",
       "      <td>2001-19-05</td>\n",
       "      <td>2001-19-05</td>\n",
       "      <td>0</td>\n",
       "      <td>64.926200</td>\n",
       "      <td>-147.158700</td>\n",
       "      <td>0.295989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2134</td>\n",
       "      <td>2001-16-07</td>\n",
       "      <td>2001-16-07</td>\n",
       "      <td>0</td>\n",
       "      <td>64.504700</td>\n",
       "      <td>-148.539500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2135</td>\n",
       "      <td>2001-28-09</td>\n",
       "      <td>2001-28-09</td>\n",
       "      <td>0</td>\n",
       "      <td>64.133300</td>\n",
       "      <td>-146.508500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2136</td>\n",
       "      <td>2002-19-05</td>\n",
       "      <td>2002-19-05</td>\n",
       "      <td>0</td>\n",
       "      <td>64.164200</td>\n",
       "      <td>-146.352200</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>4258</td>\n",
       "      <td>2022-29-08</td>\n",
       "      <td>2022-29-08</td>\n",
       "      <td>0</td>\n",
       "      <td>55.558100</td>\n",
       "      <td>-161.893500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>4259</td>\n",
       "      <td>2022-30-09</td>\n",
       "      <td>2022-30-09</td>\n",
       "      <td>0</td>\n",
       "      <td>55.562800</td>\n",
       "      <td>-161.890900</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>4260</td>\n",
       "      <td>2022-30-09</td>\n",
       "      <td>2022-30-09</td>\n",
       "      <td>0</td>\n",
       "      <td>55.560600</td>\n",
       "      <td>-161.875700</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>4261</td>\n",
       "      <td>2022-15-11</td>\n",
       "      <td>2022-17-11</td>\n",
       "      <td>0</td>\n",
       "      <td>55.558150</td>\n",
       "      <td>-161.887600</td>\n",
       "      <td>0.156672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>4262</td>\n",
       "      <td>2022-29-11</td>\n",
       "      <td>2022-02-12</td>\n",
       "      <td>0</td>\n",
       "      <td>55.558500</td>\n",
       "      <td>-161.888817</td>\n",
       "      <td>0.299791</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2131 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ClusterID DetectionDate_first DetectionDate_last  Duration  avgLatitude  \\\n",
       "0          2132          2000-28-12         2000-28-12         0    63.098125   \n",
       "1          2133          2001-19-05         2001-19-05         0    64.926200   \n",
       "2          2134          2001-16-07         2001-16-07         0    64.504700   \n",
       "3          2135          2001-28-09         2001-28-09         0    64.133300   \n",
       "4          2136          2002-19-05         2002-19-05         0    64.164200   \n",
       "...         ...                 ...                ...       ...          ...   \n",
       "2126       4258          2022-29-08         2022-29-08         0    55.558100   \n",
       "2127       4259          2022-30-09         2022-30-09         0    55.562800   \n",
       "2128       4260          2022-30-09         2022-30-09         0    55.560600   \n",
       "2129       4261          2022-15-11         2022-17-11         0    55.558150   \n",
       "2130       4262          2022-29-11         2022-02-12         0    55.558500   \n",
       "\n",
       "      avgLongitude  totalArea_km2  maxBrightness  maxBright_t31  maxFRP  \\\n",
       "0      -162.816950       0.979713              0              0       0   \n",
       "1      -147.158700       0.295989              0              0       0   \n",
       "2      -148.539500       0.250000              0              0       0   \n",
       "3      -146.508500       0.250000              0              0       0   \n",
       "4      -146.352200       0.250000              0              0       0   \n",
       "...            ...            ...            ...            ...     ...   \n",
       "2126   -161.893500       0.250000              0              0       0   \n",
       "2127   -161.890900       0.250000              0              0       0   \n",
       "2128   -161.875700       0.250000              0              0       0   \n",
       "2129   -161.887600       0.156672              0              0       0   \n",
       "2130   -161.888817       0.299791              0              0       0   \n",
       "\n",
       "      avgConfidence  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "2126              0  \n",
       "2127              0  \n",
       "2128              0  \n",
       "2129              0  \n",
       "2130              0  \n",
       "\n",
       "[2131 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta  \n",
    "import pandas as pd\n",
    "# wildfire_data = pd.read_csv('AK_Fire_Location_cleaned_v2.csv', usecols=['LATITUDE', 'LONGITUDE', 'DISCOVERYD', 'DISCOVERYS', 'ESTIMATEDT'])\n",
    "# wildfire_data = wildfire_data.dropna()\n",
    "# wildfire_data = wildfire_data[wildfire_data['LATITUDE']>=64]\n",
    "# wildfire_data = wildfire_data[pd.to_datetime(wildfire_data['DISCOVERYD']).dt.year<=2015]\n",
    "# wildfire_data\n",
    "\n",
    "wildfire_data = pd.read_csv('Task_FireDetection_noRS_negClass_V4.csv')\n",
    "wildfire_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2c3a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=iFK75-CAMzFbBwb6UaC2WOMKFS4nBHOTuno4RB8Kgkg&tc=ZQ5ARqwNcWwM55ufJYMn2yOS3VS7Hwnrx0EgJllj4c4&cc=pQsWKesykRNa3pgbqlZ8_v-tiQK5VEwze7pk2xjDefw>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=iFK75-CAMzFbBwb6UaC2WOMKFS4nBHOTuno4RB8Kgkg&tc=ZQ5ARqwNcWwM55ufJYMn2yOS3VS7Hwnrx0EgJllj4c4&cc=pQsWKesykRNa3pgbqlZ8_v-tiQK5VEwze7pk2xjDefw</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AeaYSHDV6BkRyNxOyY6eUk8PT-_b0wwJWZg9osmV2ZYoikVnUIGmVRmGu5A\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "# Trigger the authentication flow.\n",
    "ee.Authenticate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedc54a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI,EVI,sur_refl_b01,sur_refl_b02,sur_refl_b03,sur_refl_b07,dayl,prcp,srad,tmax,tmin,LST_Day_1km,LST_Night_1km,Emis_31,Clear_day_cov,Clear_night_cov,Albedo_BSA_nir,Albedo_BSA_shortwave,Albedo_WSA_nir,Albedo_WSA_shortwave,"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "ee.Initialize()  # Initialize the Earth Engine module.\n",
    "\n",
    "def get_time_series(collection, start_date, end_date, feature_name, poi):\n",
    "    \"\"\"\n",
    "    Retrieve time series data for the specified period.\n",
    "    \"\"\"\n",
    "    def extract_data(image):\n",
    "        value = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=350).get(feature_name)\n",
    "        date = image.date().format()\n",
    "        return ee.Feature(None, {'date': date, feature_name: value})\n",
    "\n",
    "    time_series = collection.filterDate(start_date, end_date).map(extract_data)\n",
    "\n",
    "    # Get the data as a list of features\n",
    "    data_list = time_series.getInfo()['features']\n",
    "    \n",
    "    # Initialize lists for dates and values\n",
    "    dates = []\n",
    "    values = []\n",
    "\n",
    "    # Process the list to extract dates and values\n",
    "    for data in data_list:\n",
    "        properties = data['properties']\n",
    "        if feature_name in properties and properties[feature_name] is not None:\n",
    "            dates.append(properties['date'])\n",
    "            values.append(properties[feature_name])\n",
    "\n",
    "    return dates, values\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    'NDVI': ee.ImageCollection('MODIS/006/MOD13Q1').select('NDVI'),\n",
    "    'EVI': ee.ImageCollection('MODIS/006/MOD13Q1').select('EVI'),\n",
    "    'sur_refl_b01': ee.ImageCollection('MODIS/006/MOD13Q1').select('sur_refl_b01'),\n",
    "    'sur_refl_b02': ee.ImageCollection('MODIS/006/MOD13Q1').select('sur_refl_b02'),\n",
    "    'sur_refl_b03': ee.ImageCollection('MODIS/006/MOD13Q1').select('sur_refl_b03'),\n",
    "    'sur_refl_b07': ee.ImageCollection('MODIS/006/MOD13Q1').select('sur_refl_b07'),\n",
    "\n",
    "    'dayl': ee.ImageCollection('NASA/ORNL/DAYMET_V4').select('dayl'),\n",
    "    'prcp': ee.ImageCollection('NASA/ORNL/DAYMET_V4').select('prcp'),\n",
    "    'srad': ee.ImageCollection('NASA/ORNL/DAYMET_V4').select('srad'),\n",
    "    'tmax': ee.ImageCollection('NASA/ORNL/DAYMET_V4').select('tmax'),\n",
    "    'tmin': ee.ImageCollection('NASA/ORNL/DAYMET_V4').select('tmin'),\n",
    "\n",
    "    'LST_Day_1km': ee.ImageCollection('MODIS/006/MOD11A1').select('LST_Day_1km'),\n",
    "    'LST_Night_1km': ee.ImageCollection('MODIS/006/MOD11A1').select('LST_Night_1km'),\n",
    "    'Emis_31': ee.ImageCollection('MODIS/006/MOD11A1').select('Emis_31'),\n",
    "    'Emis_32': ee.ImageCollection('MODIS/006/MOD11A1').select('Emis_32'),\n",
    "    'Clear_day_cov': ee.ImageCollection('MODIS/006/MOD11A1').select('Clear_day_cov'),\n",
    "    'Clear_night_cov': ee.ImageCollection('MODIS/006/MOD11A1').select('Clear_night_cov'),\n",
    "\n",
    "    'Albedo_BSA_nir': ee.ImageCollection('MODIS/061/MCD43A3').select('Albedo_BSA_nir'),\n",
    "    'Albedo_BSA_shortwave': ee.ImageCollection('MODIS/061/MCD43A3').select('Albedo_BSA_shortwave'),\n",
    "    'Albedo_WSA_nir': ee.ImageCollection('MODIS/061/MCD43A3').select('Albedo_WSA_nir'),\n",
    "    'Albedo_WSA_shortwave': ee.ImageCollection('MODIS/061/MCD43A3').select('Albedo_WSA_shortwave'),\n",
    "\n",
    "#     'NDSI_Snow_Cover': ee.ImageCollection('MODIS/061/MOD10A1').select('NDSI_Snow_Cover'),\n",
    "#     'Nadir_Reflectance_Band1': ee.ImageCollection('MODIS/006/MCD43A4').select('Nadir_Reflectance_Band1'),\n",
    "\n",
    "#     'Fpar': ee.ImageCollection('MODIS/061/MCD15A3H').select('Fpar'),\n",
    "}\n",
    "\n",
    "# Load your dataset\n",
    "# Replace with the path to your dataset file\n",
    "df = pd.read_csv('Task_FireDetection_noRS_negClass_V4.csv')\n",
    "df['DetectionDate_first'] = pd.to_datetime(df['DetectionDate_first'], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Process each row in the dataset\n",
    "# Process and save each feature's time series\n",
    "for feature_name, collection in datasets.items():\n",
    "    print(f'{feature_name}', end=',')\n",
    "    all_dates = []\n",
    "    all_values = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "#         print(f'{index}', end=',')\n",
    "        lat, lon, fire_date_str = row['avgLatitude'], row['avgLongitude'], row['DetectionDate_first']\n",
    "        fire_date = pd.to_datetime(fire_date_str)\n",
    "        poi = ee.Geometry.Point(lon, lat)\n",
    "        start_date = fire_date - pd.DateOffset(days=90)\n",
    "        end_date = fire_date + pd.DateOffset(days=1)\n",
    "\n",
    "        # Get time series data for this feature and fire event\n",
    "        try:\n",
    "            dates, values = get_time_series(collection, start_date, end_date, feature_name, poi)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing location ({lat}, {lon}) for feature {feature_name}: {str(e)}\")\n",
    "\n",
    "        # Add the fire date at the beginning of each list\n",
    "        dates.insert(0, row['DetectionDate_first'])\n",
    "        values.insert(0, row['DetectionDate_first'])\n",
    "#         print(f'{values}', end=',')\n",
    "\n",
    "        # Append these lists to the respective all_dates and all_values lists\n",
    "        all_dates.append(dates)\n",
    "        all_values.append(values)\n",
    "\n",
    "    # Create DataFrames from the lists of all dates and all values\n",
    "    dates_df = pd.DataFrame(all_dates)\n",
    "    values_df = pd.DataFrame(all_values)\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    dates_df.to_csv(f'{feature_name}_negClassV4_dates.csv', index=False)\n",
    "    values_df.to_csv(f'{feature_name}_negClassV4_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3932125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered Elevation, Slope and Aspect!\n"
     ]
    }
   ],
   "source": [
    "#ELEVATION DATA\n",
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Earth Engine module\n",
    "ee.Initialize()\n",
    "\n",
    "# Function to get elevation and population\n",
    "def get_topographical_features(lat, lon):\n",
    "    try:\n",
    "        point = ee.Geometry.Point([lon, lat])\n",
    "        \n",
    "        # Load the elevation data\n",
    "        elevation_dataset = ee.ImageCollection('JAXA/ALOS/AW3D30/V3_2').select('DSM').mean()\n",
    "\n",
    "        # Calculate slope and aspect\n",
    "        slope = ee.Terrain.slope(elevation_dataset)\n",
    "        aspect = ee.Terrain.aspect(elevation_dataset)\n",
    "\n",
    "        # Reduce region to get values\n",
    "        elev_value = elevation_dataset.reduceRegion(ee.Reducer.mean(), point, scale=350).get('DSM').getInfo()\n",
    "        slope_value = slope.reduceRegion(ee.Reducer.mean(), point, scale=350).get('slope').getInfo()\n",
    "        aspect_value = aspect.reduceRegion(ee.Reducer.mean(), point, scale=350).get('aspect').getInfo()\n",
    "\n",
    "        return [\n",
    "            elev_value if elev_value is not None else pd.NA,\n",
    "            slope_value if slope_value is not None else pd.NA,\n",
    "            aspect_value if aspect_value is not None else pd.NA\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        return [pd.NA, pd.NA, pd.NA, str(e)]  # Return NA for all values and the error message\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv('Task_FireDetection_noRS_negClass_V4.csv')\n",
    "\n",
    "# Initialize columns for elevation, slope, and aspect\n",
    "df['Elevation'] = pd.NA\n",
    "df['Slope'] = pd.NA\n",
    "df['Aspect'] = pd.NA\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    lat, lon = row['avgLatitude'], row['avgLongitude']\n",
    "    elev,slo,asp = get_topographical_features(lat, lon)\n",
    "#     print(elev,slo,asp)\n",
    "    \n",
    "    # Store results in DataFrame\n",
    "    df.at[index, 'Elevation'] = elev\n",
    "    df.at[index, 'Slope'] = slo\n",
    "    df.at[index, 'Aspect'] = asp\n",
    "    \n",
    "df.to_csv('Elevation_Slope_Aspect_negClassV4_values.csv', index=False)\n",
    "print('Gathered Elevation, Slope and Aspect!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e30afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC_Type1,LC_Type2,LC_Type3,LC_Type4,LC_Type5,"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "ee.Initialize()  # Initialize the Earth Engine module.\n",
    "\n",
    "def get_LC(lat, lon, year, lc_type):\n",
    "    try:\n",
    "        point = ee.Geometry.Point([lon, lat])\n",
    "        # Load the MODIS land cover image collection\n",
    "        LC_collection = ee.ImageCollection(\"MODIS/061/MCD12Q1\").select(lc_type)\n",
    "        # Filter the collection for the specified year\n",
    "        LC_image = LC_collection.filter(ee.Filter.calendarRange(year, year, 'year')).first()\n",
    "        # Reduce the region around the point\n",
    "        LC_value = LC_image.reduceRegion(\n",
    "            ee.Reducer.mode(), \n",
    "            point.buffer(1000),  # 1km buffer around the point\n",
    "            scale=100  # Scale in meters; adjust as necessary\n",
    "        ).get(lc_type).getInfo()\n",
    "\n",
    "        return LC_value if LC_value is not None else 'No Data'\n",
    "    except Exception as e:\n",
    "        return f'Error: {str(e)}'\n",
    "\n",
    "# Land cover types to process\n",
    "lc_types = ['LC_Type1','LC_Type2', 'LC_Type3', 'LC_Type4', 'LC_Type5']  # 'LC_Type1', \n",
    "\n",
    "# Process each row in the dataset for each land cover type\n",
    "for lc_type in lc_types:\n",
    "    # Load your dataset\n",
    "    df = pd.read_csv('Task_FireDetection_noRS_negClass_V4.csv')\n",
    "    df['DetectionDate_first'] = pd.to_datetime(df['DetectionDate_first'], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "    df['Year'] = pd.to_datetime(df['DetectionDate_first']).dt.year\n",
    "\n",
    "    print(f'{lc_type}', end=',')\n",
    "    for index, row in df.iterrows():\n",
    "        lat, lon, year = row['avgLatitude'], row['avgLongitude'], row['Year']\n",
    "        lc_value = get_LC(lat, lon, year, lc_type)\n",
    "\n",
    "        # Store results in DataFrame\n",
    "        df.at[index, lc_type] = lc_value\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    df.to_csv(f'{lc_type}_negClassV4_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "ee.Initialize()  # Initialize the Earth Engine module.\n",
    "\n",
    "def get_time_series(collection, start_date, end_date, feature_name, poi):\n",
    "    \"\"\"\n",
    "    Retrieve time series data for the specified period.\n",
    "    \"\"\"\n",
    "    def extract_data(image):\n",
    "        value = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=350).get(feature_name)\n",
    "        date = image.date().format()\n",
    "        return ee.Feature(None, {'date': date, feature_name: value})\n",
    "\n",
    "    time_series = collection.filterDate(start_date, end_date).map(extract_data)\n",
    "\n",
    "    # Get the data as a list of features\n",
    "    data_list = time_series.getInfo()['features']\n",
    "    \n",
    "    # Initialize lists for dates and values\n",
    "    dates = []\n",
    "    values = []\n",
    "\n",
    "    # Process the list to extract dates and values\n",
    "    for data in data_list:\n",
    "        properties = data['properties']\n",
    "        if feature_name in properties and properties[feature_name] is not None:\n",
    "            dates.append(properties['date'])\n",
    "            values.append(properties[feature_name])\n",
    "\n",
    "    return dates, values\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "#     'v_component_of_wind_10m': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('v_component_of_wind_10m'),\n",
    "    'u_component_of_wind_10m': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('u_component_of_wind_10m'),\n",
    "    'surface_pressure': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('surface_pressure'),\n",
    "    'runoff_sum': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('runoff_sum'),\n",
    "    'total_evaporation_sum': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('total_evaporation_sum'),\n",
    "    'snowfall_sum': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('snowfall_sum'),\n",
    "    'snowmelt_sum': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('snowmelt_sum'),\n",
    "    'snow_depth': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('snow_depth'),\n",
    "    'snow_density': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('snow_density'),\n",
    "    'snow_cover': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('snow_cover'),\n",
    "    'dewpoint_temperature_2m': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('dewpoint_temperature_2m'),\n",
    "    'temperature_2m': ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR').select('temperature_2m')\n",
    "}\n",
    "\n",
    "# Load your dataset\n",
    "# Replace with the path to your dataset file\n",
    "df = pd.read_csv('Task_FireDetection_noRS_negClass_V4.csv')\n",
    "df['DetectionDate_first'] = pd.to_datetime(df['DetectionDate_first'], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Process each row in the dataset\n",
    "# Process and save each feature's time series\n",
    "for feature_name, collection in datasets.items():\n",
    "    print(f'{feature_name}', end=',')\n",
    "    all_dates = []\n",
    "    all_values = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "#         print(f'{index}', end=',')\n",
    "        lat, lon, fire_date_str = row['avgLatitude'], row['avgLongitude'], row['DetectionDate_first']\n",
    "        fire_date = pd.to_datetime(fire_date_str)\n",
    "        poi = ee.Geometry.Point(lon, lat)\n",
    "        start_date = fire_date - pd.DateOffset(days=90)\n",
    "        end_date = fire_date + pd.DateOffset(days=1)\n",
    "\n",
    "        # Get time series data for this feature and fire event\n",
    "        dates, values = get_time_series(collection, start_date, end_date, feature_name, poi)\n",
    "\n",
    "        # Add the fire date at the beginning of each list\n",
    "        dates.insert(0, row['DetectionDate_first'])\n",
    "        values.insert(0, row['DetectionDate_first'])\n",
    "\n",
    "        # Append these lists to the respective all_dates and all_values lists\n",
    "        all_dates.append(dates)\n",
    "        all_values.append(values)\n",
    "        \n",
    "#         print(values)\n",
    "    # Create DataFrames from the lists of all dates and all values\n",
    "    dates_df = pd.DataFrame(all_dates)\n",
    "    values_df = pd.DataFrame(all_values)\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    dates_df.to_csv(f'{feature_name}_negClassV4_dates.csv', index=False)\n",
    "    values_df.to_csv(f'{feature_name}_negClassV4_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653f60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd480f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a41e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13031669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984e3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db85ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aba1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
